import asyncio
import os
from dotenv import load_dotenv
from llm_providers import DoubaoProvider, DeepSeekProvider, QwenProvider

load_dotenv()

async def test_debate_scenario():
    print("=" * 60)
    print("æµ‹è¯•è±†åŒ…åœ¨å¤šè½®è¾©è®ºåœºæ™¯ä¸­çš„è¡¨ç°")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿè¾©è®ºå†å²
    chat_history = [
        {"role": "user", "name": "User", "content": "äººå·¥æ™ºèƒ½æ˜¯å¦ä¼šå–ä»£äººç±»å·¥ä½œï¼Ÿ"},
        {"role": "assistant", "name": "DeepSeek", "content": "æˆ‘è®¤ä¸ºAIä¼šå–ä»£éƒ¨åˆ†é‡å¤æ€§å·¥ä½œï¼Œä½†åˆ›é€ æ€§å·¥ä½œéš¾ä»¥æ›¿ä»£ã€‚"},
        {"role": "assistant", "name": "Qwen", "content": "ä¸å®Œå…¨åŒæ„ã€‚AIå¯èƒ½ä¼šæ”¹å˜å·¥ä½œå½¢å¼ï¼Œä½†ä¼šåˆ›é€ æ–°å²—ä½ã€‚"},
    ]
    
    provider = DoubaoProvider()
    
    print(f"\nğŸ¤– æµ‹è¯• {provider.name}")
    print(f"ğŸ“ å½“å‰å¯¹è¯å†å²: {len(chat_history)} æ¡æ¶ˆæ¯")
    print("\n" + "=" * 60)
    print("è±†åŒ…çš„å›å¤:")
    print("=" * 60)
    
    try:
        full_response = ""
        chunk_count = 0
        
        async for chunk in provider.stream_response(chat_history):
            full_response += chunk
            chunk_count += 1
            print(chunk, end="", flush=True)
        
        print("\n" + "=" * 60)
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"   - æ”¶åˆ°å—æ•°: {chunk_count}")
        print(f"   - æ€»å­—ç¬¦æ•°: {len(full_response)}")
        print(f"   - æ˜¯å¦åªæœ‰è¡¨æƒ…: {'âŒ æ˜¯' if len(full_response) < 10 else 'âœ… å¦'}")
        
        if chunk_count < 5:
            print(f"\nâš ï¸  è­¦å‘Š: å—æ•°å¤ªå°‘ ({chunk_count})ï¼Œå¯èƒ½å‡ºç°é—®é¢˜!")
            
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_debate_scenario())
